{"cells":[{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21740,"status":"ok","timestamp":1717737664474,"user":{"displayName":"이규원","userId":"12166977002529024931"},"user_tz":-540},"id":"vy_cxxiPGASD","outputId":"fdd8e566-08e9-49ac-a4bd-a052836d2ed3"},"outputs":[],"source":["# from google.colab import drive\n","# drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":19,"metadata":{"id":"P-mO2T97JFcc"},"outputs":[],"source":["# path = '/content/drive/MyDrive/Colab Notebooks/dataset'\n","path = 'C:/dataset'"]},{"cell_type":"code","execution_count":20,"metadata":{"id":"cS3z95H6JQ_0"},"outputs":[],"source":["# import zipfile\n","# zip_file = zipfile.ZipFile(path+'/osteoarthritis.zip') # 압축을 해제할 '/파일경로/파일명.zip'\n","# zip_file.extractall('/content/drive/MyDrive/Colab Notebooks/dataset/')"]},{"cell_type":"code","execution_count":21,"metadata":{"id":"f6aSkgS8J7WH"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import numpy as np\n","import os\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import torchvision.models as models\n","import math\n","import cv2"]},{"cell_type":"code","execution_count":22,"metadata":{"id":"aeCqL7lzLFwm"},"outputs":[],"source":["categories = ['train', 'test', 'val', 'auto_test'] # 전처리된 데이터셋을 훈련용, 평가용, 검증용으로 구분\n","data_dir = path+'/osteoarthritis/'\n","# device = torch.device('mps:0' if torch.backends.mps.is_available() else 'cpu') # Mac OS\n","device = torch.device( 'cuda' if torch.cuda.is_available() else 'cpu' )"]},{"cell_type":"code","execution_count":23,"metadata":{"id":"tMT7NjG9KrZJ"},"outputs":[],"source":["def resize_image(img,size=(128,128)):\n","    return cv2.resize(img,size)\n","\n","def he_img(img):\n","    return cv2.equalizeHist(img)\n","\n","def clahe_image(img):\n","    clahe = cv2.createCLAHE(clipLimit=2.,tileGridSize=(8,8))\n","    cl_img = clahe.apply(img)\n","    return cl_img\n","\n","def denoise_img(img):\n","    return cv2.fastNlMeansDenoising(img,None,30,7,21)\n","\n","def normalize_img(img):\n","    return cv2.normalize(img,None,0,255,cv2.NORM_MINMAX)\n","\n","def detect_edge(img):\n","    return cv2.Canny(img,100,200)\n","\n","def blur_img(img):\n","    return cv2.GaussianBlur(img,(5,5),0)\n","\n","def find_contour(img):\n","    ret, thresh = cv2.threshold(img, 127, 255, 0)\n","    contours, hiearchy = cv2.findContours(thresh, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n","    return contours"]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[],"source":["# 데이터 증강"]},{"cell_type":"code","execution_count":25,"metadata":{"id":"_XY2BWU-K4xp"},"outputs":[],"source":["def load_data(data_dir):\n","    images = []\n","    for img_name in os.listdir(data_dir):\n","        img_path = os.path.join(data_dir, img_name)\n","        if os.path.isfile(img_path):  # 파일인지 확인\n","            img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n","            if img is not None:  # 이미지가 정상적으로 로드되었는지 확인\n","                img = resize_image(img)\n","                img = clahe_image(img)\n","                img = normalize_img(img)\n","                images.append(img)\n","    prepared_data = np.array(images)\n","    return prepared_data\n","\n","# 증강 or 클래스 별 동일한 숫자\n","# 전체적으로 분류할 때 비율을 맞춰보는게..\n","# 증강(rotation, zoomin)\n"]},{"cell_type":"code","execution_count":26,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":115869,"status":"ok","timestamp":1717738329831,"user":{"displayName":"이규원","userId":"12166977002529024931"},"user_tz":-540},"id":"qqDsPg2gK_CZ","outputId":"de9576f3-1c50-4d90-879c-12db48aabb17"},"outputs":[{"name":"stdout","output_type":"stream","text":["카테고리: train, 라벨: 0 처리 중....\n","처리된 이미지 수: 2286\n","========================================\n","카테고리: train, 라벨: 1 처리 중....\n","처리된 이미지 수: 1046\n","========================================\n","카테고리: train, 라벨: 2 처리 중....\n","처리된 이미지 수: 1516\n","========================================\n","카테고리: train, 라벨: 3 처리 중....\n","처리된 이미지 수: 757\n","========================================\n","카테고리: train, 라벨: 4 처리 중....\n","처리된 이미지 수: 173\n","========================================\n","카테고리: test, 라벨: 0 처리 중....\n","처리된 이미지 수: 639\n","========================================\n","카테고리: test, 라벨: 1 처리 중....\n","처리된 이미지 수: 296\n","========================================\n","카테고리: test, 라벨: 2 처리 중....\n","처리된 이미지 수: 447\n","========================================\n","카테고리: test, 라벨: 3 처리 중....\n","처리된 이미지 수: 223\n","========================================\n","카테고리: test, 라벨: 4 처리 중....\n","처리된 이미지 수: 51\n","========================================\n","카테고리: val, 라벨: 0 처리 중....\n","처리된 이미지 수: 328\n","========================================\n","카테고리: val, 라벨: 1 처리 중....\n","처리된 이미지 수: 153\n","========================================\n","카테고리: val, 라벨: 2 처리 중....\n","처리된 이미지 수: 212\n","========================================\n","카테고리: val, 라벨: 3 처리 중....\n","처리된 이미지 수: 106\n","========================================\n","카테고리: val, 라벨: 4 처리 중....\n","처리된 이미지 수: 27\n","========================================\n","카테고리: auto_test, 라벨: 0 처리 중....\n","처리된 이미지 수: 604\n","========================================\n","카테고리: auto_test, 라벨: 1 처리 중....\n","처리된 이미지 수: 275\n","========================================\n","카테고리: auto_test, 라벨: 2 처리 중....\n","처리된 이미지 수: 403\n","========================================\n","카테고리: auto_test, 라벨: 3 처리 중....\n","처리된 이미지 수: 200\n","========================================\n","카테고리: auto_test, 라벨: 4 처리 중....\n","처리된 이미지 수: 44\n","========================================\n"]}],"source":["all_data = {}\n","\n","# 각 카테고리와 라벨에 따라 이미지를 처리\n","for category in categories:\n","    category_path = os.path.join(data_dir, category)\n","    all_data[category] = {}\n","    for label in range(5):\n","        label_path = os.path.join(category_path, str(label))\n","        if os.path.isdir(label_path):  # 라벨 경로가 디렉토리인지 확인\n","            print(f\"카테고리: {category}, 라벨: {label} 처리 중....\")\n","            processed_img_list = load_data(label_path)\n","            all_data[category][label] = processed_img_list\n","            print(f\"처리된 이미지 수: {len(processed_img_list)}\")\n","            print('='*40)"]},{"cell_type":"code","execution_count":27,"metadata":{"id":"WFTyhCa1R-EO"},"outputs":[],"source":["import numpy as np\n","from sklearn.model_selection import train_test_split\n","import torch\n","from torch.utils.data import DataLoader, TensorDataset\n","\n","\n","# train, val, test 데이터를 모두 합침\n","combined_data = []\n","combined_labels = []\n","\n","for dataset in ['train', 'val', 'test']:\n","    for label, images in all_data[dataset].items():\n","        combined_data.append(images)\n","        combined_labels.append(np.full(images.shape[0], label))\n","\n","combined_data = np.concatenate(combined_data, axis=0)\n","combined_labels = np.concatenate(combined_labels, axis=0)\n","\n","# 80/10/10 비율로 train, val, test 세트를 나눔\n","train_data, test_data, train_labels, test_labels = train_test_split(combined_data, combined_labels, test_size=0.2, random_state=42)\n","val_data, test_data, val_labels, test_labels = train_test_split(test_data, test_labels, test_size=0.5, random_state=42)\n","\n","# PyTorch 텐서로 변환\n","train_data_tensor = torch.tensor(train_data, dtype=torch.float32)\n","train_labels_tensor = torch.tensor(train_labels, dtype=torch.long)\n","val_data_tensor = torch.tensor(val_data, dtype=torch.float32)\n","val_labels_tensor = torch.tensor(val_labels, dtype=torch.long)\n","test_data_tensor = torch.tensor(test_data, dtype=torch.float32)\n","test_labels_tensor = torch.tensor(test_labels, dtype=torch.long)\n","\n","# PyTorch 데이터셋 및 데이터 로더 생성\n","train_dataset = TensorDataset(train_data_tensor, train_labels_tensor)\n","val_dataset = TensorDataset(val_data_tensor, val_labels_tensor)\n","test_dataset = TensorDataset(test_data_tensor, test_labels_tensor)\n","\n","train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n","val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n","test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"]},{"cell_type":"code","execution_count":28,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1717741457515,"user":{"displayName":"이규원","userId":"12166977002529024931"},"user_tz":-540},"id":"Aw9LJJbJUarh","outputId":"51fbeac4-7905-4baf-8fca-5b1550cf1b39"},"outputs":[{"data":{"text/plain":["((6608, 128, 128), (826, 128, 128), (6608,), (826,))"]},"execution_count":28,"metadata":{},"output_type":"execute_result"}],"source":["train_data.shape, test_data.shape, train_labels.shape, test_labels.shape"]},{"cell_type":"code","execution_count":29,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1717741460186,"user":{"displayName":"이규원","userId":"12166977002529024931"},"user_tz":-540},"id":"MQpIZVdtUucs","outputId":"65a96da7-6088-4b23-9fd4-ffcf3e738bb4"},"outputs":[{"data":{"text/plain":["((826, 128, 128), (826, 128, 128), (826,), (826,))"]},"execution_count":29,"metadata":{},"output_type":"execute_result"}],"source":["val_data.shape, test_data.shape, val_labels.shape, test_labels.shape"]},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[{"ename":"RuntimeError","evalue":"Given groups=1, weight of size [64, 3, 7, 7], expected input[1, 32, 128, 128] to have 3 channels, but got 32 channels instead","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[1;32mIn[30], line 60\u001b[0m\n\u001b[0;32m     57\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mrepeat(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     59\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 60\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     61\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m     62\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n","File \u001b[1;32mc:\\Users\\igyuw\\anaconda3\\envs\\tf\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n","File \u001b[1;32mc:\\Users\\igyuw\\anaconda3\\envs\\tf\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[1;32mc:\\Users\\igyuw\\anaconda3\\envs\\tf\\lib\\site-packages\\torchvision\\models\\resnet.py:285\u001b[0m, in \u001b[0;36mResNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 285\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[1;32mc:\\Users\\igyuw\\anaconda3\\envs\\tf\\lib\\site-packages\\torchvision\\models\\resnet.py:268\u001b[0m, in \u001b[0;36mResNet._forward_impl\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_forward_impl\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m    267\u001b[0m     \u001b[38;5;66;03m# See note [TorchScript super()]\u001b[39;00m\n\u001b[1;32m--> 268\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    269\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn1(x)\n\u001b[0;32m    270\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(x)\n","File \u001b[1;32mc:\\Users\\igyuw\\anaconda3\\envs\\tf\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n","File \u001b[1;32mc:\\Users\\igyuw\\anaconda3\\envs\\tf\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[1;32mc:\\Users\\igyuw\\anaconda3\\envs\\tf\\lib\\site-packages\\torch\\nn\\modules\\conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[1;32mc:\\Users\\igyuw\\anaconda3\\envs\\tf\\lib\\site-packages\\torch\\nn\\modules\\conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[0;32m    454\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[0;32m    455\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[1;32m--> 456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    457\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[1;31mRuntimeError\u001b[0m: Given groups=1, weight of size [64, 3, 7, 7], expected input[1, 32, 128, 128] to have 3 channels, but got 32 channels instead"]}],"source":["# Define the number of output classes\n","num_classes = 5\n","\n","# Load pre-trained models and modify the final layer for transfer learning\n","def get_pretrained_model(model_name, num_classes):\n","    if model_name == 'resnet':\n","        model = models.resnet50(pretrained=True)\n","        model.fc = nn.Linear(model.fc.in_features, num_classes)\n","    elif model_name == 'densenet':\n","        model = models.densenet121(pretrained=True)\n","        model.classifier = nn.Linear(model.classifier.in_features, num_classes)\n","    elif model_name == 'vgg':\n","        model = models.vgg16(pretrained=True)\n","        model.classifier[6] = nn.Linear(model.classifier[6].in_features, num_classes)\n","    else:\n","        raise ValueError('Unknown model name')\n","    \n","    return model\n","\n","# 사전학습 모델 설정\n","criterion = nn.CrossEntropyLoss()\n","model_name = 'resnet'  # or 'densenet' or 'vgg'\n","model = get_pretrained_model(model_name, num_classes)\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","model = model.to(device)\n","\n","# Freeze initial layers\n","for param in model.parameters():\n","    param.requires_grad = False\n","\n","# Unfreeze the last layer\n","if model_name == 'resnet':\n","    for param in model.fc.parameters():\n","        param.requires_grad = True\n","elif model_name == 'densenet':\n","    for param in model.classifier.parameters():\n","        param.requires_grad = True\n","elif model_name == 'vgg':\n","    for param in model.classifier[6].parameters():\n","        param.requires_grad = True\n","\n","# Redefine optimizer to update only the last layer\n","optimizer = optim.Adam(filter(lambda x: x.requires_grad, model.parameters()), lr=0.001)\n","\n","# Training loop\n","num_epochs = 30\n","for epoch in range(num_epochs):\n","    model.train()\n","    running_loss = 0.0\n","    for i, data in enumerate(train_loader, 0):\n","        inputs, labels = data\n","        inputs, labels = inputs.to(device), labels.to(device)\n","        \n","        # Ensure inputs have the correct shape (N, C, H, W) and convert grayscale to RGB\n","        if inputs.ndim == 3:\n","            inputs = inputs.unsqueeze(1)  # Add channel dimension if missing\n","        if inputs.shape[1] == 1:\n","            inputs = inputs.repeat(1, 3, 1, 1)\n","        \n","        optimizer.zero_grad()\n","        outputs = model(inputs)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","\n","        running_loss += loss.item()\n","        if i % 100 == 99:\n","            print(f'[Epoch {epoch + 1}, Batch {i + 1}] loss: {running_loss / 100:.3f}')\n","            running_loss = 0.0\n","\n","    model.eval()\n","    val_loss = 0.0\n","    correct = 0\n","    total = 0\n","    with torch.no_grad():\n","        for data in val_loader:\n","            images, labels = data\n","            images, labels = images.to(device), labels.to(device)\n","            \n","            # Ensure images have the correct shape (N, C, H, W) and convert grayscale to RGB\n","            if images.ndim == 3:\n","                images = images.unsqueeze(1)  # Add channel dimension if missing\n","            if images.shape[1] == 1:\n","                images = images.repeat(1, 3, 1, 1)\n","            \n","            outputs = model(images)\n","            loss = criterion(outputs, labels)\n","            val_loss += loss.item()\n","            _, predicted = torch.max(outputs.data, 1)\n","            total += labels.size(0)\n","            correct += (predicted == labels).sum().item()\n","\n","    print(f'Epoch {epoch + 1}, Validation loss: {val_loss / len(val_loader):.3f}, Accuracy: {100 * correct / total:.2f}%')\n","\n","print('Training complete')\n","\n","# Testing phase\n","model.eval()\n","correct = 0\n","total = 0\n","with torch.no_grad():\n","    for data in test_loader:\n","        images, labels = data\n","        images, labels = images.to(device), labels.to(device)\n","        \n","        # Ensure images have the correct shape (N, C, H, W) and convert grayscale to RGB\n","        if images.ndim == 3:\n","            images = images.unsqueeze(1)  # Add channel dimension if missing\n","        if images.shape[1] == 1:\n","            images = images.repeat(1, 3, 1, 1)\n","        \n","        outputs = model(images)\n","        _, predicted = torch.max(outputs.data, 1)\n","        total += labels.size(0)\n","        correct += (predicted == labels).sum().item()\n","\n","print(f'Test Accuracy: {100 * correct / total:.2f}%')\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7Wybk5F-V7de"},"outputs":[],"source":["# # 테스트 단계\n","# model.eval()\n","# correct = 0\n","# total = 0\n","# with torch.no_grad():\n","#     for data in test_loader:\n","#         images, labels = data\n","#         inputs, labels = inputs.to(device), labels.to(device)\n","#         outputs = model(images)\n","#         _, predicted = torch.max(outputs.data, 1)\n","#         total += labels.size(0)\n","#         correct += (predicted == labels).sum().item()\n","\n","# print(f'Test Accuracy: {100 * correct / total:.2f}%')"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyM18Nh3+xFB72VFV2xhHYNL","gpuType":"T4","mount_file_id":"181UmXhZRdtJpBK4YXbMd7VE_qHlKiyek","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat":4,"nbformat_minor":0}
