{"cells":[{"cell_type":"code","execution_count":128,"metadata":{"id":"P-mO2T97JFcc"},"outputs":[],"source":["\n","path = 'C:/dataset'"]},{"cell_type":"code","execution_count":129,"metadata":{"id":"f6aSkgS8J7WH"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import numpy as np\n","import os\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import torchvision.models as models\n","import math\n","import cv2\n","from sklearn.utils import shuffle"]},{"cell_type":"code","execution_count":130,"metadata":{"id":"aeCqL7lzLFwm"},"outputs":[],"source":["categories = ['train', 'test', 'val', 'auto_test'] # 전처리된 데이터셋을 훈련용, 평가용, 검증용으로 구분\n","data_dir = path+'/osteoarthritis_aug_class3/'\n","# device = torch.device('mps:0' if torch.backends.mps.is_available() else 'cpu') # Mac OS\n","device = torch.device( 'cuda' if torch.cuda.is_available() else 'cpu' )"]},{"cell_type":"code","execution_count":131,"metadata":{"id":"tMT7NjG9KrZJ"},"outputs":[],"source":["def resize_image(img,size=(224,224)):\n","    return cv2.resize(img,size)\n","\n","def he_img(img):\n","    return cv2.equalizeHist(img)\n","\n","def clahe_image(img):\n","    clahe = cv2.createCLAHE(clipLimit=2.,tileGridSize=(8,8))\n","    cl_img = clahe.apply(img)\n","    return cl_img\n","\n","def denoise_img(img):\n","    return cv2.fastNlMeansDenoising(img,None,30,7,21)\n","\n","def normalize_img(img):\n","    return cv2.normalize(img,None,0,255,cv2.NORM_MINMAX)\n","\n","def detect_edge(img):\n","    return cv2.Canny(img,100,200)\n","\n","def blur_img(img):\n","    return cv2.GaussianBlur(img,(5,5),0)\n","\n","def find_contour(img):\n","    ret, thresh = cv2.threshold(img, 127, 255, 0)\n","    contours, hiearchy = cv2.findContours(thresh, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n","    return contours"]},{"cell_type":"code","execution_count":132,"metadata":{},"outputs":[],"source":["# import os\n","# import numpy as np\n","# from tensorflow.keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img\n","# import matplotlib.pyplot as plt\n","\n","\n","# # Data augmentation settings\n","# datagen = ImageDataGenerator(\n","#     rotation_range=3,\n","#     width_shift_range=0.05,\n","#     height_shift_range=0.05,\n","#     shear_range=0.05,\n","#     zoom_range=0.05,\n","#     fill_mode='nearest',\n","#     brightness_range=[0.3, 0.7]  # 밝기 조절 범위 설정\n","# )\n","\n","# def augment_images(data_dir, label, augment_count=600):\n","#     label_path = os.path.join(data_dir, 'train', str(label))\n","#     augmented_dir = os.path.join(label_path, 'augmented')\n","#     os.makedirs(augmented_dir, exist_ok=True)\n","\n","#     img_names = os.listdir(label_path)\n","#     img_names = [name for name in img_names if os.path.isfile(os.path.join(label_path, name))]\n","#     generated_count = 0\n","\n","#     while generated_count < augment_count:\n","#         img_name = np.random.choice(img_names)\n","#         img_path = os.path.join(label_path, img_name)\n","#         try:\n","#             img = load_img(img_path, color_mode='grayscale', target_size=(224, 224))\n","#             img = img_to_array(img)\n","#             img = img.reshape((1,) + img.shape)\n","\n","#             for batch in datagen.flow(img, batch_size=1, save_to_dir=augmented_dir, save_prefix='aug', save_format='png'):\n","#                 generated_count += 1\n","#                 break  # 한 장만 생성하고 반복문 탈출\n","#         except PermissionError:\n","#             print(f\"Permission denied: Unable to access file {img_path}\")\n","#         except Exception as e:\n","#             print(f\"Error processing file {img_path}: {e}\")\n","\n","# # Augment images in label 4\n","# # data_dir = path+'/osteoarthritis/train/'  # 적절한 데이터 디렉토리로 변경\n","# augment_images(data_dir, 4)\n","\n","# # Display augmented images\n","# augmented_img_paths = os.listdir(os.path.join(data_dir, 'train', '4', 'augmented'))\n","# augmented_img_paths = shuffle(augmented_img_paths)[:6]  # 무작위로 6개 선택\n","\n","# for i, img_name in enumerate(augmented_img_paths):\n","#     img_path = os.path.join(data_dir, 'train', '4', 'augmented', img_name)\n","#     img = load_img(img_path, color_mode='grayscale', target_size=(128, 128))\n","#     plt.imshow(img, cmap='gray')\n","#     plt.title(f'Augmented Image {i + 1}')\n","#     plt.show()\n"]},{"cell_type":"code","execution_count":133,"metadata":{"id":"_XY2BWU-K4xp"},"outputs":[],"source":["def load_data(data_dir):\n","    images = []\n","    for img_name in os.listdir(data_dir):\n","        img_path = os.path.join(data_dir, img_name)\n","        if os.path.isfile(img_path):  # 파일인지 확인\n","            img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n","            if img is not None:  # 이미지가 정상적으로 로드되었는지 확인\n","                img = resize_image(img)\n","                img = clahe_image(img)\n","                img = normalize_img(img)\n","                images.append(img)\n","    prepared_data = np.array(images)\n","    return prepared_data\n","\n","def load_and_augment_data(data_dir, label):\n","    label_path = os.path.join(data_dir, 'train', str(label))\n","    augmented_dir = os.path.join(label_path, 'augmented')\n","\n","    images = []\n","    # Load original images\n","    for img_name in os.listdir(label_path):\n","        if img_name.endswith('.png'):\n","            img_path = os.path.join(label_path, img_name)\n","            img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n","            if img is not None:\n","                img = resize_image(img)\n","                img = clahe_image(img)\n","                img = normalize_img(img)\n","                images.append(img)\n","    \n","    # Load augmented images\n","    if os.path.exists(augmented_dir):\n","        for img_name in os.listdir(augmented_dir):\n","            img_path = os.path.join(augmented_dir, img_name)\n","            img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n","            if img is not None:\n","                img = resize_image(img)\n","                img = clahe_image(img)\n","                img = normalize_img(img)\n","                images.append(img)\n","    \n","    return np.array(images)\n","\n","# 증강 or 클래스 별 동일한 숫자\n","# 전체적으로 분류할 때 비율을 맞춰보는게..\n","# 증강(rotation, zoomin)\n"]},{"cell_type":"code","execution_count":134,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":115869,"status":"ok","timestamp":1717738329831,"user":{"displayName":"이규원","userId":"12166977002529024931"},"user_tz":-540},"id":"qqDsPg2gK_CZ","outputId":"de9576f3-1c50-4d90-879c-12db48aabb17"},"outputs":[],"source":["# 각 카테고리와 라벨에 따라 이미지를 처리\n","all_data = {}\n","for category in categories:\n","    category_path = os.path.join(data_dir, category)\n","    all_data[category] = {}\n","    for label in range(3):\n","        if category == 'train' and label == 2:\n","            processed_img_list = load_and_augment_data(data_dir, label)\n","        else:\n","            label_path = os.path.join(category_path, str(label))\n","            if os.path.isdir(label_path):\n","                processed_img_list = load_data(label_path)\n","            else:\n","                processed_img_list = np.array([])\n","        all_data[category][label] = processed_img_list\n"]},{"cell_type":"code","execution_count":135,"metadata":{},"outputs":[],"source":["# Function to sample 500 images from each label for training\n","def sample_images(data, num_samples=300):\n","    sampled_data = []\n","    sampled_labels = []\n","    for label, images in data.items():\n","        if len(images) >= num_samples:\n","            sampled_data.append(images[:num_samples])\n","            sampled_labels.append(np.full(num_samples, label))\n","        else:\n","            sampled_data.append(images)\n","            sampled_labels.append(np.full(len(images), label))\n","    sampled_data = np.concatenate(sampled_data, axis=0)\n","    sampled_labels = np.concatenate(sampled_labels, axis=0)\n","    return shuffle(sampled_data, sampled_labels)"]},{"cell_type":"code","execution_count":136,"metadata":{},"outputs":[],"source":["# Sample 500 images\n","train_data_sampled, train_labels_sampled = sample_images(all_data['train'])\n","val_data_sampled, val_labels_sampled = sample_images(all_data['val'])\n","test_data_sampled, test_labels_sampled = sample_images(all_data['test'])"]},{"cell_type":"code","execution_count":137,"metadata":{"id":"WFTyhCa1R-EO"},"outputs":[],"source":["import numpy as np\n","from sklearn.model_selection import train_test_split\n","import torch\n","from torch.utils.data import DataLoader, TensorDataset\n","\n","val_data_combined = []\n","val_labels_combined = []\n","test_data_combined = []\n","test_labels_combined = []\n","\n","for dataset in ['val', 'test']:\n","    for label, images in all_data[dataset].items():\n","        if dataset == 'val':\n","            val_data_combined.append(images)\n","            val_labels_combined.append(np.full(images.shape[0], label))\n","        else:\n","            test_data_combined.append(images)\n","            test_labels_combined.append(np.full(images.shape[0], label))\n","\n","val_data_combined = np.concatenate(val_data_combined, axis=0)\n","val_labels_combined = np.concatenate(val_labels_combined, axis=0)\n","test_data_combined = np.concatenate(test_data_combined, axis=0)\n","test_labels_combined = np.concatenate(test_labels_combined, axis=0)\n","\n","# PyTorch 텐서로 변환\n","train_data_tensor = torch.tensor(train_data_sampled, dtype=torch.float32)\n","train_labels_tensor = torch.tensor(train_labels_sampled, dtype=torch.long)\n","val_data_tensor = torch.tensor(val_data_combined, dtype=torch.float32)\n","val_labels_tensor = torch.tensor(val_labels_combined, dtype=torch.long)\n","test_data_tensor = torch.tensor(test_data_combined, dtype=torch.float32)\n","test_labels_tensor = torch.tensor(test_labels_combined, dtype=torch.long)\n","\n","# PyTorch 데이터셋 및 데이터 로더 생성\n","train_dataset = TensorDataset(train_data_tensor, train_labels_tensor)\n","val_dataset = TensorDataset(val_data_tensor, val_labels_tensor)\n","test_dataset = TensorDataset(test_data_tensor, test_labels_tensor)\n","\n","train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n","val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n","test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"]},{"cell_type":"code","execution_count":138,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1717741457515,"user":{"displayName":"이규원","userId":"12166977002529024931"},"user_tz":-540},"id":"Aw9LJJbJUarh","outputId":"51fbeac4-7905-4baf-8fca-5b1550cf1b39"},"outputs":[{"data":{"text/plain":["(torch.Size([900, 224, 224]),\n"," torch.Size([900]),\n"," torch.Size([1656, 224, 224]),\n"," torch.Size([1656]))"]},"execution_count":138,"metadata":{},"output_type":"execute_result"}],"source":["train_data_tensor.shape,train_labels_tensor.shape ,test_data_tensor.shape, test_labels_tensor.shape"]},{"cell_type":"code","execution_count":139,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1717741460186,"user":{"displayName":"이규원","userId":"12166977002529024931"},"user_tz":-540},"id":"MQpIZVdtUucs","outputId":"65a96da7-6088-4b23-9fd4-ffcf3e738bb4"},"outputs":[{"data":{"text/plain":["(torch.Size([826, 224, 224]),\n"," torch.Size([1656, 224, 224]),\n"," torch.Size([826]),\n"," torch.Size([1656]))"]},"execution_count":139,"metadata":{},"output_type":"execute_result"}],"source":["val_data_tensor.shape, test_data_tensor.shape, val_labels_tensor.shape, test_labels_tensor.shape"]},{"cell_type":"code","execution_count":140,"metadata":{},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torchvision import models\n","\n","# Define the number of output classes\n","num_classes = 3\n","\n","def get_in_features(model_name):\n","    if model_name == 'resnet':\n","        model = models.resnet50(pretrained=True)\n","        in_features = model.fc.in_features\n","    elif model_name == 'densenet':\n","        model = models.densenet121(pretrained=True)\n","        in_features = model.classifier.in_features\n","    elif model_name == 'vgg':\n","        model = models.vgg16(pretrained=True)\n","        in_features = model.classifier[6].in_features\n","    else:\n","        raise ValueError('Unknown model name')\n","    return in_features\n","\n","class CustomResNet(nn.Module):\n","    def __init__(self, num_classes, in_features):\n","        super(CustomResNet, self).__init__()\n","        self.resnet = models.resnet50(pretrained=True)\n","        self.resnet.fc = nn.Identity()\n","        self.global_avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n","        self.dropout = nn.Dropout(0.2)\n","        self.fc = nn.Linear(in_features, num_classes)\n","\n","    def forward(self, x):\n","        x = self.resnet(x)\n","        x = self.global_avg_pool(x)\n","        x = torch.flatten(x, 1)\n","        x = self.dropout(x)\n","        x = self.fc(x)\n","        return x\n","\n","class CustomDenseNet(nn.Module):\n","    def __init__(self, num_classes, in_features):\n","        super(CustomDenseNet, self).__init__()\n","        self.densenet = models.densenet121(pretrained=True)\n","        self.densenet.classifier = nn.Identity()\n","        self.global_avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n","        self.dropout = nn.Dropout(0.2)\n","        self.fc = nn.Linear(in_features, num_classes)\n","\n","    def forward(self, x):\n","        x = self.densenet.features(x)\n","        x = self.global_avg_pool(x)\n","        x = torch.flatten(x, 1)\n","        x = self.dropout(x)\n","        x = self.fc(x)\n","        return x\n","\n","class CustomVGG(nn.Module):\n","    def __init__(self, num_classes, in_features):\n","        super(CustomVGG, self).__init__()\n","        self.vgg = models.vgg16(pretrained=True)\n","        self.vgg.classifier = nn.Identity()\n","        self.global_avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n","        self.dropout = nn.Dropout(0.2)\n","        self.fc = nn.Linear(in_features, num_classes)\n","\n","    def forward(self, x):\n","        x = self.vgg.features(x)\n","        x = self.global_avg_pool(x)\n","        x = torch.flatten(x, 1)\n","        x = self.dropout(x)\n","        x = self.fc(x)\n","        return x\n"]},{"cell_type":"code","execution_count":141,"metadata":{"id":"plZErbJ4U1xx"},"outputs":[{"ename":"ValueError","evalue":"Input dimension should be at least 3","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[1;32mIn[141], line 48\u001b[0m\n\u001b[0;32m     45\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mrepeat(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     47\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 48\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     49\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m     50\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n","File \u001b[1;32mc:\\Users\\igyuw\\anaconda3\\envs\\tf\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n","File \u001b[1;32mc:\\Users\\igyuw\\anaconda3\\envs\\tf\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","Cell \u001b[1;32mIn[140], line 36\u001b[0m, in \u001b[0;36mCustomResNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     35\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresnet(x)\n\u001b[1;32m---> 36\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mglobal_avg_pool\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mflatten(x, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     38\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(x)\n","File \u001b[1;32mc:\\Users\\igyuw\\anaconda3\\envs\\tf\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n","File \u001b[1;32mc:\\Users\\igyuw\\anaconda3\\envs\\tf\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[1;32mc:\\Users\\igyuw\\anaconda3\\envs\\tf\\lib\\site-packages\\torch\\nn\\modules\\pooling.py:1267\u001b[0m, in \u001b[0;36mAdaptiveAvgPool2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m   1266\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m-> 1267\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madaptive_avg_pool2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_size\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[1;32mc:\\Users\\igyuw\\anaconda3\\envs\\tf\\lib\\site-packages\\torch\\nn\\functional.py:1259\u001b[0m, in \u001b[0;36madaptive_avg_pool2d\u001b[1;34m(input, output_size)\u001b[0m\n\u001b[0;32m   1257\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m   1258\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(adaptive_avg_pool2d, (\u001b[38;5;28minput\u001b[39m,), \u001b[38;5;28minput\u001b[39m, output_size)\n\u001b[1;32m-> 1259\u001b[0m _output_size \u001b[38;5;241m=\u001b[39m \u001b[43m_list_with_default\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1260\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39madaptive_avg_pool2d(\u001b[38;5;28minput\u001b[39m, _output_size)\n","File \u001b[1;32mc:\\Users\\igyuw\\anaconda3\\envs\\tf\\lib\\site-packages\\torch\\nn\\modules\\utils.py:38\u001b[0m, in \u001b[0;36m_list_with_default\u001b[1;34m(out_size, defaults)\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out_size\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(defaults) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(out_size):\n\u001b[1;32m---> 38\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     39\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput dimension should be at least \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(out_size)\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     40\u001b[0m     )\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m     42\u001b[0m     v \u001b[38;5;28;01mif\u001b[39;00m v \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m d \u001b[38;5;28;01mfor\u001b[39;00m v, d \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(out_size, defaults[\u001b[38;5;241m-\u001b[39m\u001b[38;5;28mlen\u001b[39m(out_size) :])\n\u001b[0;32m     43\u001b[0m ]\n","\u001b[1;31mValueError\u001b[0m: Input dimension should be at least 3"]}],"source":["# Choose the model\n","model_name = 'resnet'  # or 'densenet' or 'vgg'\n","in_features = get_in_features(model_name)\n","\n","if model_name == 'resnet':\n","    model = CustomResNet(num_classes, in_features)\n","elif model_name == 'densenet':\n","    model = CustomDenseNet(num_classes, in_features)\n","elif model_name == 'vgg':\n","    model = CustomVGG(num_classes, in_features)\n","else:\n","    raise ValueError('Unknown model name')\n","\n","# Setup training\n","criterion = nn.CrossEntropyLoss()\n","model = model.to(device)\n","\n","# Freeze initial layers\n","for param in model.parameters():\n","    param.requires_grad = False\n","\n","# Unfreeze the last layers\n","for param in model.fc.parameters():\n","    param.requires_grad = True\n","\n","# Define optimizer to update only the last layer\n","optimizer = optim.SGD(filter(lambda x: x.requires_grad, model.parameters()), lr=0.001, momentum=0.9, weight_decay=0.0005)\n","\n","# Training loop\n","num_epochs = 30\n","for epoch in range(num_epochs):\n","    model.train()\n","    running_loss = 0.0\n","    correct_train = 0\n","    total_train = 0\n","    for i, data in enumerate(train_loader, 0):\n","        inputs, labels = data\n","        inputs, labels = inputs.to(device), labels.to(device)\n","        \n","        # Ensure inputs have the correct shape (N, C, H, W) and convert grayscale to RGB\n","        if inputs.ndim == 3:\n","            inputs = inputs.unsqueeze(1)  # Add channel dimension if missing\n","        if inputs.shape[1] == 1:\n","            inputs = inputs.repeat(1, 3, 1, 1)\n","        \n","        optimizer.zero_grad()\n","        outputs = model(inputs)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","\n","        running_loss += loss.item()\n","        _, predicted = torch.max(outputs.data, 1)\n","        total_train += labels.size(0)\n","        correct_train += (predicted == labels).sum().item()\n","        \n","        if i % 100 == 99:\n","            print(f'[Epoch {epoch + 1}, Batch {i + 1}] loss: {running_loss / 100:.3f}')\n","            running_loss = 0.0\n","\n","    train_loss = running_loss / len(train_loader)\n","    train_accuracy = 100 * correct_train / total_train\n","    \n","    model.eval()\n","    val_loss = 0.0\n","    correct_val = 0\n","    total_val = 0\n","    with torch.no_grad():\n","        for data in val_loader:\n","            images, labels = data\n","            images, labels = images.to(device), labels.to(device)\n","            \n","            if images.ndim == 3:\n","                images = images.unsqueeze(1)\n","            if images.shape[1] == 1:\n","                images = images.repeat(1, 3, 1, 1)\n","            \n","            outputs = model(images)\n","            loss = criterion(outputs, labels)\n","            val_loss += loss.item()\n","            _, predicted = torch.max(outputs.data, 1)\n","            total_val += labels.size(0)\n","            correct_val += (predicted == labels).sum().item()\n","\n","    val_loss /= len(val_loader)\n","    val_accuracy = 100 * correct_val / total_val\n","    \n","    print(f'Epoch {epoch + 1}, Train loss: {train_loss:.3f}, Train Accuracy: {train_accuracy:.2f}%')\n","    print(f'Epoch {epoch + 1}, Validation loss: {val_loss:.3f}, Validation Accuracy: {val_accuracy:.2f}%')\n","    print('Training complete')\n","\n","# Testing phase\n","model.eval()\n","correct = 0\n","total = 0\n","all_labels = []\n","all_predictions = []\n","with torch.no_grad():\n","    for data in test_loader:\n","        images, labels = data\n","        images, labels = images.to(device), labels.to(device)\n","                                                   \n","        if images.ndim == 3:\n","            images = images.unsqueeze(1)  # Add channel dimension if missing\n","        if images.shape[1] == 1:\n","            images = images.repeat(1, 3, 1, 1)\n","        \n","        outputs = model(images)\n","        _, predicted = torch.max(outputs.data, 1)\n","        total += labels.size(0)\n","        correct += (predicted == labels).sum().item()\n","        all_labels.extend(labels.cpu().numpy())\n","        all_predictions.extend(predicted.cpu().numpy())\n","\n","test_accuracy = 100 * correct / total\n","print(f'Test Accuracy: {test_accuracy:.2f}%')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Confusion Matrix:\n","[[1064  316    2]\n"," [  84  133    6]\n"," [   4   45    2]]\n"]},{"data":{"image/png":"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","text/plain":["<Figure size 1000x800 with 2 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["from sklearn.metrics import confusion_matrix\n","\n","# Confusion matrix\n","conf_matrix = confusion_matrix(all_labels, all_predictions)\n","print('Confusion Matrix:')\n","print(conf_matrix)\n","\n","# Plotting confusion matrix\n","plt.figure(figsize=(10, 8))\n","sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=range(num_classes), yticklabels=range(num_classes))\n","plt.xlabel('Predicted')\n","plt.ylabel('True')\n","plt.title('Confusion Matrix')\n","plt.show()\n","\n","# 0 : 639\n","# 1 : 296\n","# 2 : 447\n","# 3 : 223\n","# 4 : 51"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# # Define the number of output classes\n","# num_classes = 5\n","\n","# # Load pre-trained models and modify the final layer for transfer learning\n","# def get_pretrained_model(model_name, num_classes):\n","#     if model_name == 'resnet':\n","#         model = models.resnet18(pretrained=True)\n","#         model.fc = nn.Linear(model.fc.in_features, num_classes)\n","#     elif model_name == 'densenet':\n","#         model = models.densenet121(pretrained=True)\n","#         model.classifier = nn.Linear(model.classifier.in_features, num_classes)\n","#     elif model_name == 'vgg':\n","#         model = models.vgg16(pretrained=True)\n","#         model.classifier[6] = nn.Linear(model.classifier[6].in_features, num_classes)\n","#     else:\n","#         raise ValueError('Unknown model name')\n","    \n","#     return model\n","\n","# # 사전학습 모델 설정\n","# model_name = 'resnet'  # or 'densenet' or 'vgg'\n","# model = get_pretrained_model(model_name, num_classes)\n","# model = model.to(device)\n","\n","# # Freeze initial layers\n","# for param in model.parameters():\n","#     param.requires_grad = False\n","\n","# # Unfreeze the last layer\n","# if model_name == 'resnet':\n","#     for param in model.fc.parameters():\n","#         param.requires_grad = True\n","# elif model_name == 'densenet':\n","#     for param in model.classifier.parameters():\n","#         param.requires_grad = True\n","# elif model_name == 'vgg':\n","#     for param in model.classifier[6].parameters():\n","#         param.requires_grad = True\n","\n","# # Redefine optimizer to update only the last layer\n","# optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=0.001)\n","\n","# # Training loop (same as before)\n","# num_epochs = 30\n","# for epoch in range(num_epochs):\n","#     model.train()\n","#     running_loss = 0.0\n","#     for i, data in enumerate(train_loader, 0):\n","#         inputs, labels = data\n","#         inputs, labels = inputs.to(device), labels.to(device)\n","#         inputs = inputs.unsqueeze(1)  # Add channel dimension for grayscale images\n","#         optimizer.zero_grad()\n","#         outputs = model(inputs)\n","#         loss = criterion(outputs, labels)\n","#         loss.backward()\n","#         optimizer.step()\n","\n","#         running_loss += loss.item()\n","#         if i % 100 == 99:\n","#             print(f'[Epoch {epoch + 1}, Batch {i + 1}] loss: {running_loss / 100:.3f}')\n","#             running_loss = 0.0\n","\n","#     model.eval()\n","#     val_loss = 0.0\n","#     correct = 0\n","#     total = 0\n","#     with torch.no_grad():\n","#         for data in val_loader:\n","#             images, labels = data\n","#             inputs, labels = inputs.to(device), labels.to(device)\n","#             images = images.unsqueeze(1)\n","#             outputs = model(images)\n","#             loss = criterion(outputs, labels)\n","#             val_loss += loss.item()\n","#             _, predicted = torch.max(outputs.data, 1)\n","#             total += labels.size(0)\n","#             correct += (predicted == labels).sum().item()\n","\n","#     print(f'Epoch {epoch + 1}, Validation loss: {val_loss / len(val_loader):.3f}, Accuracy: {100 * correct / total:.2f}%')\n","\n","# print('Training complete')\n","\n","# # Testing phase (same as before)\n","# model.eval()\n","# correct = 0\n","# total = 0\n","# with torch.no_grad():\n","#     for data in test_loader:\n","#         images, labels = data\n","#         inputs, labels = inputs.to(device), labels.to(device)\n","#         outputs = model(images)\n","#         _, predicted = torch.max(outputs.data, 1)\n","#         total += labels.size(0)\n","#         correct += (predicted == labels).sum().item()\n","\n","# print(f'Test Accuracy: {100 * correct / total:.2f}%')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7Wybk5F-V7de"},"outputs":[],"source":["# # 테스트 단계\n","# model.eval()\n","# correct = 0\n","# total = 0\n","# with torch.no_grad():\n","#     for data in test_loader:\n","#         images, labels = data\n","#         inputs, labels = inputs.to(device), labels.to(device)\n","#         outputs = model(images)\n","#         _, predicted = torch.max(outputs.data, 1)\n","#         total += labels.size(0)\n","#         correct += (predicted == labels).sum().item()\n","\n","# print(f'Test Accuracy: {100 * correct / total:.2f}%')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyM18Nh3+xFB72VFV2xhHYNL","gpuType":"T4","mount_file_id":"181UmXhZRdtJpBK4YXbMd7VE_qHlKiyek","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat":4,"nbformat_minor":0}
